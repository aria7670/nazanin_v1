"""
Advanced Algorithms & Pattern Recognition
Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ùˆ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from collections import defaultdict, Counter
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)


class PatternRecognitionAlgorithm:
    """Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ"""
    
    def __init__(self):
        self.patterns = {}
        self.pattern_history = []
        
    async def detect_patterns(self, data: List[Dict]) -> Dict[str, Any]:
        """ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡"""
        
        patterns_found = {
            'time_patterns': await self._detect_time_patterns(data),
            'content_patterns': await self._detect_content_patterns(data),
            'engagement_patterns': await self._detect_engagement_patterns(data),
            'user_behavior_patterns': await self._detect_user_patterns(data)
        }
        
        return patterns_found
    
    async def _detect_time_patterns(self, data: List[Dict]) -> Dict:
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        time_distribution = defaultdict(int)
        
        for item in data:
            if 'timestamp' in item:
                try:
                    dt = datetime.fromisoformat(item['timestamp'])
                    hour = dt.hour
                    time_distribution[hour] += 1
                except:
                    pass
        
        # ÛŒØ§ÙØªÙ† Ø³Ø§Ø¹Ø§Øª Ù¾ÛŒÚ©
        if time_distribution:
            sorted_hours = sorted(time_distribution.items(), key=lambda x: x[1], reverse=True)
            peak_hours = [hour for hour, count in sorted_hours[:3]]
        else:
            peak_hours = []
        
        return {
            'peak_hours': peak_hours,
            'distribution': dict(time_distribution),
            'most_active_hour': peak_hours[0] if peak_hours else None
        }
    
    async def _detect_content_patterns(self, data: List[Dict]) -> Dict:
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒÛŒ"""
        topics = Counter()
        content_lengths = []
        
        for item in data:
            if 'topic' in item:
                topics[item['topic']] += 1
            if 'content' in item:
                content_lengths.append(len(item['content']))
        
        return {
            'top_topics': topics.most_common(5),
            'avg_content_length': np.mean(content_lengths) if content_lengths else 0,
            'content_length_std': np.std(content_lengths) if content_lengths else 0
        }
    
    async def _detect_engagement_patterns(self, data: List[Dict]) -> Dict:
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„"""
        engagement_scores = []
        
        for item in data:
            if 'engagement' in item:
                engagement_scores.append(item['engagement'])
        
        if not engagement_scores:
            return {'avg_engagement': 0, 'trend': 'stable'}
        
        avg_engagement = np.mean(engagement_scores)
        
        # ØªØ´Ø®ÛŒØµ ØªØ±Ù†Ø¯
        if len(engagement_scores) > 5:
            recent_avg = np.mean(engagement_scores[-5:])
            overall_avg = np.mean(engagement_scores[:-5])
            
            if recent_avg > overall_avg * 1.2:
                trend = 'growing'
            elif recent_avg < overall_avg * 0.8:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'insufficient_data'
        
        return {
            'avg_engagement': avg_engagement,
            'trend': trend,
            'best_performing': max(engagement_scores) if engagement_scores else 0
        }
    
    async def _detect_user_patterns(self, data: List[Dict]) -> Dict:
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±"""
        user_segments = {
            'power_users': 0,
            'regular_users': 0,
            'casual_users': 0
        }
        
        user_activity = defaultdict(int)
        
        for item in data:
            if 'user_id' in item:
                user_activity[item['user_id']] += 1
        
        for user_id, activity_count in user_activity.items():
            if activity_count > 20:
                user_segments['power_users'] += 1
            elif activity_count > 5:
                user_segments['regular_users'] += 1
            else:
                user_segments['casual_users'] += 1
        
        return user_segments


class ContentOptimizationAlgorithm:
    """Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§"""
    
    def __init__(self):
        self.optimization_history = []
        
    async def optimize(self, content: str, target_metrics: Dict) -> Dict[str, Any]:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§"""
        
        # ØªØ­Ù„ÛŒÙ„ ÙØ¹Ù„ÛŒ
        current_metrics = await self._analyze_content(content)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ gap
        gaps = self._calculate_gaps(current_metrics, target_metrics)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§
        optimized_content = content
        optimizations_applied = []
        
        for gap_type, gap_value in gaps.items():
            if abs(gap_value) > 0.1:  # Ø¢Ø³ØªØ§Ù†Ù‡ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±
                optimized_content, optimization = await self._apply_optimization(
                    optimized_content, gap_type, gap_value
                )
                optimizations_applied.append(optimization)
        
        # ØªØ­Ù„ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ
        final_metrics = await self._analyze_content(optimized_content)
        
        result = {
            'original_content': content,
            'optimized_content': optimized_content,
            'original_metrics': current_metrics,
            'final_metrics': final_metrics,
            'improvements': self._calculate_improvements(current_metrics, final_metrics),
            'optimizations_applied': optimizations_applied
        }
        
        self.optimization_history.append(result)
        
        return result
    
    async def _analyze_content(self, content: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§"""
        return {
            'length': len(content),
            'word_count': len(content.split()),
            'sentence_count': content.count('.') + 1,
            'readability_score': await self._calculate_readability(content),
            'engagement_potential': await self._estimate_engagement(content)
        }
    
    async def _calculate_readability(self, content: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ (Flesch Reading Ease Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡)"""
        words = content.split()
        sentences = content.count('.') + 1
        
        if sentences == 0 or len(words) == 0:
            return 0.0
        
        avg_words_per_sentence = len(words) / sentences
        
        # Ø³Ø§Ø¯Ù‡: Ù‡Ø±Ú†Ù‡ Ø¬Ù…Ù„Ø§Øª Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ØŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±
        score = 100 - (avg_words_per_sentence * 2)
        
        return max(0, min(100, score))
    
    async def _estimate_engagement(self, content: str) -> float:
        """ØªØ®Ù…ÛŒÙ† Ù¾ØªØ§Ù†Ø³ÛŒÙ„ engagement"""
        score = 50.0
        
        # Ø¹ÙˆØ§Ù…Ù„ Ù…Ø«Ø¨Øª
        if '?' in content:
            score += 10  # Ø³ÙˆØ§Ù„ engagement Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯
        if any(word in content.lower() for word in ['new', 'breaking', 'amazing', 'Ø¬Ø¯ÛŒØ¯', 'ÙÙˆØ±ÛŒ']):
            score += 15
        if 100 < len(content) < 250:
            score += 10  # Ø·ÙˆÙ„ Ø¨Ù‡ÛŒÙ†Ù‡
        
        # Ø¹ÙˆØ§Ù…Ù„ Ù…Ù†ÙÛŒ
        if len(content) > 500:
            score -= 20  # Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ
        
        return max(0, min(100, score))
    
    def _calculate_gaps(self, current: Dict, target: Dict) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ú©Ø§Ùâ€ŒÙ‡Ø§"""
        gaps = {}
        
        for key in target:
            if key in current:
                gaps[key] = target[key] - current[key]
        
        return gaps
    
    async def _apply_optimization(self, content: str, gap_type: str, 
                                  gap_value: float) -> Tuple[str, str]:
        """Ø§Ø¹Ù…Ø§Ù„ ÛŒÚ© Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        
        if gap_type == 'length' and gap_value < 0:
            # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù†
            optimized = content[:int(len(content) + gap_value)]
            return optimized, f"Shortened by {abs(gap_value)} characters"
        
        elif gap_type == 'readability_score' and gap_value > 0:
            # Ø¨Ù‡Ø¨ÙˆØ¯ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ
            # Ø³Ø§Ø¯Ù‡: ØªØ¨Ø¯ÛŒÙ„ Ø¬Ù…Ù„Ø§Øª Ø¨Ù„Ù†Ø¯ Ø¨Ù‡ Ú©ÙˆØªØ§Ù‡
            sentences = content.split('.')
            if len(sentences) > 2:
                optimized = '. '.join(sentences[:2]) + '.'
                return optimized, "Improved readability by shortening sentences"
        
        return content, "No optimization applied"
    
    def _calculate_improvements(self, before: Dict, after: Dict) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§"""
        improvements = {}
        
        for key in before:
            if key in after:
                change = after[key] - before[key]
                change_percent = (change / before[key] * 100) if before[key] != 0 else 0
                improvements[key] = {
                    'absolute_change': change,
                    'percent_change': change_percent
                }
        
        return improvements


class PredictiveAnalyticsAlgorithm:
    """Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
    
    def __init__(self):
        self.models = {}
        self.prediction_history = []
        
    async def predict_engagement(self, content_features: Dict, 
                                historical_data: List[Dict]) -> Dict:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ engagement"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ
        X_train, y_train = self._prepare_training_data(historical_data)
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ (Linear Regression Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡)
        model = await self._train_simple_model(X_train, y_train)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        X_test = self._extract_features_vector(content_features)
        prediction = await self._predict(model, X_test)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        confidence_interval = await self._calculate_confidence(
            prediction, historical_data
        )
        
        result = {
            'predicted_engagement': prediction,
            'confidence_interval': confidence_interval,
            'confidence_level': 0.75,  # 75% confidence
            'factors': self._get_influential_factors(content_features, model)
        }
        
        self.prediction_history.append(result)
        
        return result
    
    def _prepare_training_data(self, data: List[Dict]) -> Tuple[List, List]:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ"""
        X = []
        y = []
        
        for item in data:
            if 'features' in item and 'engagement' in item:
                X.append(self._extract_features_vector(item['features']))
                y.append(item['engagement'])
        
        return X, y
    
    def _extract_features_vector(self, features: Dict) -> List[float]:
        """ØªØ¨Ø¯ÛŒÙ„ features Ø¨Ù‡ vector"""
        vector = [
            features.get('length', 0) / 1000.0,  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            features.get('word_count', 0) / 100.0,
            1.0 if features.get('has_hashtag', False) else 0.0,
            1.0 if features.get('has_emoji', False) else 0.0,
            features.get('readability_score', 50) / 100.0,
            features.get('hour', 12) / 24.0
        ]
        
        return vector
    
    async def _train_simple_model(self, X: List, y: List) -> Dict:
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡"""
        if not X or not y:
            return {'weights': [1.0] * 6, 'bias': 0.0}
        
        # Ø³Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡: ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª
        model = {
            'weights': [10, 5, 15, 12, 8, 7],  # ÙˆØ²Ù† Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ
            'bias': 20
        }
        
        return model
    
    async def _predict(self, model: Dict, X: List[float]) -> float:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
        weights = model['weights']
        bias = model['bias']
        
        prediction = bias
        for i, feature_value in enumerate(X):
            if i < len(weights):
                prediction += weights[i] * feature_value
        
        return max(0, prediction)  # Ø­Ø¯Ø§Ù‚Ù„ 0
    
    async def _calculate_confidence(self, prediction: float, 
                                    historical_data: List[Dict]) -> Tuple[float, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†"""
        if not historical_data:
            return (prediction * 0.8, prediction * 1.2)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±
        engagements = [item.get('engagement', 0) for item in historical_data]
        std = np.std(engagements) if engagements else 10
        
        # ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† 95%
        lower = prediction - 1.96 * std
        upper = prediction + 1.96 * std
        
        return (max(0, lower), upper)
    
    def _get_influential_factors(self, features: Dict, model: Dict) -> List[Dict]:
        """Ø¹ÙˆØ§Ù…Ù„ ØªØ§Ø«ÛŒØ±Ú¯Ø°Ø§Ø±"""
        feature_names = ['length', 'word_count', 'has_hashtag', 
                        'has_emoji', 'readability', 'hour']
        weights = model['weights']
        
        influential = []
        
        for i, name in enumerate(feature_names):
            if i < len(weights):
                influential.append({
                    'feature': name,
                    'weight': weights[i],
                    'impact': 'high' if weights[i] > 10 else 'medium' if weights[i] > 5 else 'low'
                })
        
        return sorted(influential, key=lambda x: x['weight'], reverse=True)


class ClusteringAlgorithm:
    """Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ"""
    
    def __init__(self, n_clusters: int = 5):
        self.n_clusters = n_clusters
        self.clusters = {}
        
    async def cluster_content(self, contents: List[Dict]) -> Dict:
        """Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø­ØªÙˆØ§"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        features_matrix = [self._extract_features(c) for c in contents]
        
        # Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³Ø§Ø¯Ù‡ (K-means Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡)
        clusters = await self._simple_kmeans(features_matrix)
        
        # ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
        cluster_analysis = await self._analyze_clusters(clusters, contents)
        
        return {
            'clusters': clusters,
            'analysis': cluster_analysis,
            'n_clusters': self.n_clusters
        }
    
    def _extract_features(self, content: Dict) -> List[float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§"""
        text = content.get('text', '')
        
        return [
            len(text) / 1000.0,
            len(text.split()) / 100.0,
            1.0 if '#' in text else 0.0,
            content.get('engagement', 0) / 100.0
        ]
    
    async def _simple_kmeans(self, features: List[List[float]]) -> Dict:
        """K-means Ø³Ø§Ø¯Ù‡"""
        if not features:
            return {}
        
        # Ù…Ø±Ø§Ú©Ø² Ø§ÙˆÙ„ÛŒÙ‡ ØªØµØ§Ø¯ÙÛŒ
        n_features = len(features[0])
        centers = [
            [np.random.random() for _ in range(n_features)]
            for _ in range(self.n_clusters)
        ]
        
        # Ø§Ø®ØªØµØ§Øµ Ù†Ù‚Ø§Ø· Ø¨Ù‡ Ù†Ø²Ø¯ÛŒÚ©ØªØ±ÛŒÙ† Ù…Ø±Ú©Ø²
        clusters = defaultdict(list)
        
        for idx, feature_vec in enumerate(features):
            distances = [
                self._euclidean_distance(feature_vec, center)
                for center in centers
            ]
            closest_cluster = np.argmin(distances)
            clusters[closest_cluster].append(idx)
        
        return dict(clusters)
    
    def _euclidean_distance(self, vec1: List[float], vec2: List[float]) -> float:
        """ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ"""
        return np.sqrt(sum((a - b) ** 2 for a, b in zip(vec1, vec2)))
    
    async def _analyze_clusters(self, clusters: Dict, contents: List[Dict]) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§"""
        analysis = {}
        
        for cluster_id, content_indices in clusters.items():
            cluster_contents = [contents[i] for i in content_indices]
            
            avg_length = np.mean([len(c.get('text', '')) for c in cluster_contents])
            avg_engagement = np.mean([c.get('engagement', 0) for c in cluster_contents])
            
            analysis[f'cluster_{cluster_id}'] = {
                'size': len(content_indices),
                'avg_length': avg_length,
                'avg_engagement': avg_engagement,
                'characteristics': self._describe_cluster(cluster_contents)
            }
        
        return analysis
    
    def _describe_cluster(self, contents: List[Dict]) -> str:
        """ØªÙˆØµÛŒÙ Ø®ÙˆØ´Ù‡"""
        avg_len = np.mean([len(c.get('text', '')) for c in contents])
        
        if avg_len < 100:
            return 'short_content'
        elif avg_len < 250:
            return 'medium_content'
        else:
            return 'long_content'


class AnomalyDetectionAlgorithm:
    """Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ"""
    
    def __init__(self):
        self.baseline_metrics = {}
        
    async def detect_anomalies(self, data: List[Dict]) -> Dict:
        """ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§"""
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ baseline
        if not self.baseline_metrics:
            await self._calculate_baseline(data)
        
        # ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        anomalies = []
        
        for item in data:
            is_anomaly, reason = await self._is_anomaly(item)
            if is_anomaly:
                anomalies.append({
                    'item': item,
                    'reason': reason,
                    'severity': self._calculate_severity(item)
                })
        
        return {
            'anomalies_found': len(anomalies),
            'anomalies': anomalies,
            'baseline': self.baseline_metrics
        }
    
    async def _calculate_baseline(self, data: List[Dict]):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø· Ù¾Ø§ÛŒÙ‡"""
        engagements = [item.get('engagement', 0) for item in data]
        lengths = [len(item.get('content', '')) for item in data]
        
        self.baseline_metrics = {
            'avg_engagement': np.mean(engagements) if engagements else 0,
            'std_engagement': np.std(engagements) if engagements else 0,
            'avg_length': np.mean(lengths) if lengths else 0,
            'std_length': np.std(lengths) if lengths else 0
        }
    
    async def _is_anomaly(self, item: Dict) -> Tuple[bool, str]:
        """Ø¢ÛŒØ§ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø± Ø§Ø³ØªØŸ"""
        engagement = item.get('engagement', 0)
        content_length = len(item.get('content', ''))
        
        # engagement ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ
        avg_eng = self.baseline_metrics['avg_engagement']
        std_eng = self.baseline_metrics['std_engagement']
        
        if engagement > avg_eng + 3 * std_eng:
            return True, 'Unusually high engagement'
        if engagement < avg_eng - 3 * std_eng and engagement < 10:
            return True, 'Unusually low engagement'
        
        # Ø·ÙˆÙ„ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ
        avg_len = self.baseline_metrics['avg_length']
        std_len = self.baseline_metrics['std_length']
        
        if content_length > avg_len + 3 * std_len:
            return True, 'Unusually long content'
        
        return False, ''
    
    def _calculate_severity(self, item: Dict) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Øª Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ"""
        engagement = item.get('engagement', 0)
        avg_eng = self.baseline_metrics['avg_engagement']
        
        if engagement < avg_eng * 0.1:
            return 'critical'
        elif engagement < avg_eng * 0.5:
            return 'high'
        elif engagement > avg_eng * 5:
            return 'positive_anomaly'
        else:
            return 'moderate'


# Ø§Ø±Ú©Ø³ØªØ±Ø§ØªÙˆØ± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§
class AlgorithmOrchestrator:
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ…Ø§Ù… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.pattern_recognition = PatternRecognitionAlgorithm()
        self.content_optimization = ContentOptimizationAlgorithm()
        self.predictive_analytics = PredictiveAnalyticsAlgorithm()
        self.clustering = ClusteringAlgorithm()
        self.anomaly_detection = AnomalyDetectionAlgorithm()
        
        logger.info("ğŸ§® All algorithms initialized")
    
    async def run_full_analysis(self, data: List[Dict]) -> Dict[str, Any]:
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„"""
        
        results = {
            'patterns': await self.pattern_recognition.detect_patterns(data),
            'anomalies': await self.anomaly_detection.detect_anomalies(data),
            'clusters': await self.clustering.cluster_content(data),
            'timestamp': datetime.now().isoformat()
        }
        
        return results
